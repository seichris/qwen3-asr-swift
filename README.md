# Qwen3-ASR Swift

A Swift implementation of [Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR) speech recognition model using [MLX Swift](https://github.com/ml-explore/mlx-swift) for Apple Silicon.

## Overview

Qwen3-ASR is a state-of-the-art automatic speech recognition model from Alibaba/Qwen that offers:

- **52 languages**: 30 major languages + 22 Chinese dialects
- **Excellent noise robustness**: Outperforms Whisper and GPT-4o in noisy conditions
- **Fast inference**: 92ms TTFT, RTF 0.064 at high concurrency
- **On-device**: Runs locally on Apple Silicon Macs and iPhones

## Models

| Model | Parameters | Use Case |
|-------|-----------|----------|
| Qwen3-ASR-0.6B | 600M | Efficient, on-device |
| Qwen3-ASR-1.7B | 1.7B | Best accuracy |

## Installation

### Swift Package Manager

Add to your `Package.swift`:

```swift
dependencies: [
    .package(url: "https://github.com/yourusername/qwen3-asr-swift", from: "0.1.0")
]
```

### Requirements

- macOS 14+ or iOS 17+
- Apple Silicon (M1/M2/M3/M4)
- Xcode 15+

## CLI Quickstart (macOS)

If you want to run the included CLI locally:

```bash
# One-time: install Metal toolchain (needed to build mlx.metallib)
xcodebuild -downloadComponent MetalToolchain

# Build the CLI
swift build -c release --disable-sandbox

# Build MLX Metal shader library next to the binary (fixes: "Failed to load the default metallib")
./scripts/build_mlx_metallib.sh release
```

Optional env vars:

```bash
# Use the default cache location under ~/Library/Caches
unset QWEN3_ASR_CACHE_DIR

# Or override cache root (must be writable)
export QWEN3_ASR_CACHE_DIR="$HOME/.cache"

# Enable verbose debug logs (very noisy; slows down inference)
export QWEN3_ASR_DEBUG=1

# Disable debug logs
unset QWEN3_ASR_DEBUG

# DashScope hosted realtime ASR (CLI + app)
export DASHSCOPE_API_KEY="YOUR_DASHSCOPE_KEY"
export DASHSCOPE_API_KEY_SG="YOUR_DASHSCOPE_SINGAPORE_KEY"
export DASHSCOPE_REALTIME_MODEL="qwen3-asr-flash-realtime-2026-02-10"
```

Run:

```bash
.build/release/qwen3-asr-cli --help

# Transcribe a file
.build/release/qwen3-asr-cli transcribe /path/to/audio.wav

# Legacy: a single positional argument is treated as `transcribe`
.build/release/qwen3-asr-cli Tests/Qwen3ASRTests/Resources/test_audio.wav

# Realtime microphone transcription/translation
.build/release/qwen3-asr-cli realtime --to en --from auto
```

Notes:
- Realtime output is normalized to plain text (the library strips the model's `language ...<asr_text>` wrapper before emitting events).
- Translation (`[TRANS]`) is generated by forcing the model's output language during ASR decoding (no extra service required).
- `--to` / `--from` accept common codes or names. The CLI normalizes them for the model prompt (e.g. `en` -> `English`, `zh`/`cn` -> `Chinese`).
- First time: macOS will prompt for microphone permission (grant it to your terminal app).

### Apple Translation (SwiftUI Apps)

The CLI’s `[TRANS]` uses a second ASR decode pass and is **not reliable for true translation**. If you want real translation for final segments, you can use Apple’s `Translation` framework from a SwiftUI host app and provide a `TranslationSession`:

```swift
import SwiftUI
import Translation
import Qwen3ASR

@available(macOS 15.0, iOS 18.0, *)
struct ContentView: View {
    @State private var config: TranslationSession.Configuration? = .init(
        source: .init(identifier: "zh"),
        target: .init(identifier: "en")
    )

    var body: some View {
        Text("…")
            .translationTask(config) { session in
                let model = try await Qwen3ASRModel.fromPretrained(modelId: "mlx-community/Qwen3-ASR-0.6B-4bit")
                let stream = await model.realtimeTranslate(
                    audioSource: MicrophoneAudioSource(frameSizeMs: 20),
                    options: .init(targetLanguage: "English", sourceLanguage: "Chinese"),
                    translationSession: session
                )
                for await event in stream {
                    // Handle .final + .translation events
                    _ = event
                }
            }
    }
}
```

### Demo App (iPhone + Mac)

This repo includes a SwiftUI demo app with language pickers and a mic button:

- Project: `Apps/Qwen3TranslateApp/Qwen3TranslateApp.xcodeproj`
- Runs on iPhone (iOS) and Mac (macOS)
- Requires iOS 18.0+ / macOS 15.0+ (Apple Translation)
- Hosted ASR option: set `DASHSCOPE_API_KEY` (Mainland) or `DASHSCOPE_API_KEY_SG` (Singapore) in your Xcode Run scheme, then choose `DashScope Mainland` or `DashScope Singapore` in the ASR dropdown.
- You can also paste these API keys in the app’s `Settings → API Keys` section (used when launching from Home Screen without Xcode-injected env vars).
- Audio input option: choose `Microphone` or `Device Audio` in the Audio dropdown (`Device Audio` uses ReplayKit on iOS).
- Hosted API reference (used by the app): [Qwen3 ASR Flash Realtime model page](https://bailian.console.alibabacloud.com/cn-beijing/?tab=model#/model-market/detail/qwen3-asr-flash-realtime-2026-02-10)
- Hosted API reference (Singapore workspace): [Qwen3 ASR Flash Realtime docs](https://modelstudio.console.alibabacloud.com/ap-southeast-1?tab=doc#/doc/?type=model&url=2840914_2&modelId=group-qwen3-asr-flash-realtime)

## Usage

### Basic Transcription

```swift
import Qwen3ASR

// Load model
let model = try await Qwen3ASRModel.fromPretrained(modelId: "Qwen/Qwen3-ASR-0.6B")

// Transcribe audio (24kHz mono float samples)
let transcription = model.transcribe(audio: audioSamples, sampleRate: 24000)
print(transcription)
```

### Realtime Translation

```swift
import Qwen3ASR

// Create audio source (microphone)
let audioSource = MicrophoneAudioSource(frameSizeMs: 20)

// Configure options
let options = RealtimeTranslationOptions(
	    targetLanguage: "en",
	    sourceLanguage: nil,
	    windowSeconds: 10.0,
	    stepMs: 500,
	    enableVAD: true
	)

// Start realtime translation
let stream = await model.realtimeTranslate(
    audioSource: audioSource,
    options: options
)

for await event in stream {
    switch event.kind {
    case .partial:
        print("Transcribing: \(event.transcript)")
    case .final:
        print("Final: \(event.transcript)")
    case .translation:
        print("Translation: \(event.translation ?? "")")
    default: break
    }
}
```

### CLI Tool

```bash
# Build CLI
swift build -c release

# If using Google Cloud Translation, set your API key first
export QWEN3_ASR_GOOGLE_TRANSLATE_API_KEY="YOUR_API_KEY"

# Transcribe audio file
qwen3-asr-cli transcribe audio.wav

# Realtime microphone translation (Google)
qwen3-asr-cli realtime --from cn --to en --translate-provider google

# Realtime microphone transcription (DashScope hosted)
qwen3-asr-cli realtime-hosted --from zh --show-partials

# Japanese translation with JSONL output
qwen3-asr-cli realtime --to ja --window 15 --jsonl

# Transcription only (no translation)
qwen3-asr-cli realtime --to en --no-translate

# Show partial transcripts ([…] / [✓]) in realtime mode
qwen3-asr-cli realtime --from cn --to en --translate-provider google --show-partials
```

## Architecture

```
Audio Input (24kHz)
    │
    ▼
┌─────────────────┐
│  Mel Spectrogram│  (WhisperFeatureExtractor)
│  128 bins, 8ms  │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Audio Encoder  │  (Conv2D + Transformer)
│  12/18 layers   │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Projector     │  (2-layer MLP)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  Text Decoder   │  (Qwen3 LLM)
│  28 layers      │
└────────┬────────┘
         │
         ▼
     Text Output
```

## Performance

### Benchmarks (Apple M3 Max)

| Model | TTFT | RTF |
|-------|------|-----|
| Qwen3-ASR-0.6B | ~100ms | ~0.08 |
| Qwen3-ASR-1.7B | ~200ms | ~0.15 |

### Word Error Rate

| Model | LibriSpeech (clean) | Noisy Conditions |
|-------|---------------------|------------------|
| Qwen3-ASR-0.6B | 2.11% | 17.88% |
| Qwen3-ASR-1.7B | 1.63% | 16.17% |
| Whisper-large-v3 | 1.51% | 63.17% |

## Supported Languages

### Major Languages (30)
Chinese, English, Cantonese, Arabic, German, French, Spanish, Portuguese, Indonesian, Italian, Korean, Russian, Thai, Vietnamese, Japanese, Turkish, Hindi, Malay, Dutch, Swedish, Danish, Finnish, Polish, Czech, Filipino, Persian, Greek, Hungarian, Macedonian, Romanian

### Chinese Dialects (22)
Anhui, Dongbei, Fujian, Gansu, Guizhou, Hebei, Henan, Hubei, Hunan, Jiangxi, Ningxia, Shandong, Shaanxi, Shanxi, Sichuan, Tianjin, Yunnan, Zhejiang, Cantonese (HK/Guangdong), Wu, Minnan

## Development Status

- [x] Configuration classes
- [x] Audio encoder (Conv2D + Transformer)
- [x] Text decoder (Qwen3)
- [x] Audio preprocessing (Mel spectrogram)
- [x] Weight loading infrastructure
- [ ] HuggingFace model download
- [ ] Tokenizer integration
- [ ] Streaming inference optimization
- [ ] iOS support

## License

Apache 2.0 (same as original Qwen3-ASR)

## Credits

- [Qwen3-ASR](https://github.com/QwenLM/Qwen3-ASR) - Original model by Alibaba/Qwen
- [MLX Swift](https://github.com/ml-explore/mlx-swift) - Apple's ML framework for Swift
- [mlx-audio](https://github.com/ml-explore/mlx-audio) - Reference Python implementation
